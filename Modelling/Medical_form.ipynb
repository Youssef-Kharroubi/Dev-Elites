{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a5f5041-47c0-4aec-a37f-7edd307938ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\easyocr\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import easyocr\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "import torch\n",
    "from PIL import Image\n",
    "import json\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8bd9401-1803-4d86-951b-ff86a7c2a9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize EasyOCR for Arabic and French\n",
    "# reader = easyocr.Reader(['ar', 'en'])\n",
    "\n",
    "# # Initialize TrOCR for handwritten text recognition\n",
    "# processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-large-handwritten\")\n",
    "# model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-large-handwritten\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c947e4-b3bc-4ea0-a891-a4917f746c47",
   "metadata": {},
   "source": [
    "## Detection of bounding box using extraction of french and arabic sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7134788-6caf-4305-93c1-7dd34d0412f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize EasyOCR for Arabic and French\n",
    "# reader = easyocr.Reader(['ar', 'en'])  # Changed 'en' to 'fr' since the form has French labels\n",
    "\n",
    "# # Step 1: Preprocess the image\n",
    "# def preprocess_image(image_path):\n",
    "#     image = cv2.imread(image_path)\n",
    "#     gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "#     thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
    "#     blur = cv2.GaussianBlur(thresh, (3, 3), 0)\n",
    "#     cv2.imwrite(\"preprocessed_form.jpg\", blur)\n",
    "#     return blur, image\n",
    "\n",
    "# # Step 2: Extract text using EasyOCR\n",
    "# def extract_text_with_easyocr(image_path):\n",
    "#     result = reader.readtext(image_path, detail=1)\n",
    "#     extracted_data = []\n",
    "#     for (bbox, text, confidence) in result:\n",
    "#         center_x = (bbox[0][0] + bbox[2][0]) / 2\n",
    "#         center_y = (bbox[0][1] + bbox[2][1]) / 2\n",
    "#         extracted_data.append({\n",
    "#             \"text\": text,\n",
    "#             \"bbox\": bbox,\n",
    "#             \"center_x\": center_x,\n",
    "#             \"center_y\": center_y,\n",
    "#             \"confidence\": confidence\n",
    "#         })\n",
    "#     return extracted_data\n",
    "\n",
    "# # Step 3: Merge adjacent text regions\n",
    "# def merge_adjacent_regions(extracted_data, y_threshold=5, x_threshold=10):\n",
    "#     if not extracted_data:\n",
    "#         return extracted_data\n",
    "    \n",
    "#     # Sort by center_y and then center_x to process regions from left to right, top to bottom\n",
    "#     sorted_data = sorted(extracted_data, key=lambda x: (x[\"center_y\"], x[\"center_x\"]))\n",
    "#     merged_data = []\n",
    "#     i = 0\n",
    "#     while i < len(sorted_data):\n",
    "#         current = sorted_data[i]\n",
    "#         merged_text = current[\"text\"]\n",
    "#         merged_bbox = current[\"bbox\"]\n",
    "#         merged_confidence = [current[\"confidence\"]]\n",
    "#         merged_center_x = [current[\"center_x\"]]\n",
    "#         merged_center_y = [current[\"center_y\"]]\n",
    "        \n",
    "#         j = i + 1\n",
    "#         while j < len(sorted_data):\n",
    "#             next_item = sorted_data[j]\n",
    "            \n",
    "#             # Check if the two regions are on the same line (within y_threshold)\n",
    "#             y_diff = abs(next_item[\"center_y\"] - current[\"center_y\"])\n",
    "#             if y_diff > y_threshold:\n",
    "#                 break\n",
    "            \n",
    "#             # Check if the two regions are horizontally close (within x_threshold)\n",
    "#             right_edge_current = current[\"bbox\"][1][0]  # Right edge of current bbox\n",
    "#             left_edge_next = next_item[\"bbox\"][0][0]  # Left edge of next bbox\n",
    "#             x_diff = left_edge_next - right_edge_current\n",
    "#             if x_diff > x_threshold:\n",
    "#                 break\n",
    "            \n",
    "#             # Merge the regions\n",
    "#             merged_text += \" \" + next_item[\"text\"]\n",
    "#             # Create a new bounding box that encompasses both regions\n",
    "#             merged_bbox = [\n",
    "#                 [min(merged_bbox[0][0], next_item[\"bbox\"][0][0]), min(merged_bbox[0][1], next_item[\"bbox\"][0][1])],\n",
    "#                 [max(merged_bbox[1][0], next_item[\"bbox\"][1][0]), min(merged_bbox[1][1], next_item[\"bbox\"][1][1])],\n",
    "#                 [max(merged_bbox[2][0], next_item[\"bbox\"][2][0]), max(merged_bbox[2][1], next_item[\"bbox\"][2][1])],\n",
    "#                 [min(merged_bbox[3][0], next_item[\"bbox\"][3][0]), max(merged_bbox[3][1], next_item[\"bbox\"][3][1])]\n",
    "#             ]\n",
    "#             merged_confidence.append(next_item[\"confidence\"])\n",
    "#             merged_center_x.append(next_item[\"center_x\"])\n",
    "#             merged_center_y.append(next_item[\"center_y\"])\n",
    "#             j += 1\n",
    "        \n",
    "#         # Calculate new center coordinates and average confidence\n",
    "#         new_center_x = sum(merged_center_x) / len(merged_center_x)\n",
    "#         new_center_y = sum(merged_center_y) / len(merged_center_y)\n",
    "#         avg_confidence = sum(merged_confidence) / len(merged_confidence)\n",
    "        \n",
    "#         merged_data.append({\n",
    "#             \"text\": merged_text,\n",
    "#             \"bbox\": merged_bbox,\n",
    "#             \"center_x\": new_center_x,\n",
    "#             \"center_y\": new_center_y,\n",
    "#             \"confidence\": avg_confidence\n",
    "#         })\n",
    "#         i = j if j > i + 1 else i + 1\n",
    "    \n",
    "#     return merged_data\n",
    "\n",
    "# # Step 4: Calculate string similarity using Levenshtein distance\n",
    "# def levenshtein_distance(s1, s2):\n",
    "#     if len(s1) < len(s2):\n",
    "#         return levenshtein_distance(s2, s1)\n",
    "    \n",
    "#     if len(s2) == 0:\n",
    "#         return len(s1)\n",
    "    \n",
    "#     previous_row = range(len(s2) + 1)\n",
    "#     for i, c1 in enumerate(s1):\n",
    "#         current_row = [i + 1]\n",
    "#         for j, c2 in enumerate(s2):\n",
    "#             insertions = previous_row[j + 1] + 1\n",
    "#             deletions = current_row[j] + 1\n",
    "#             substitutions = previous_row[j] + (c1 != c2)\n",
    "#             current_row.append(min(insertions, deletions, substitutions))\n",
    "#         previous_row = current_row\n",
    "    \n",
    "#     return previous_row[-1]\n",
    "\n",
    "# def string_similarity(s1, s2):\n",
    "#     distance = levenshtein_distance(s1.lower(), s2.lower())\n",
    "#     max_len = max(len(s1), len(s2))\n",
    "#     if max_len == 0:\n",
    "#         return 1.0\n",
    "#     similarity = 1 - (distance / max_len)\n",
    "#     return similarity\n",
    "\n",
    "# # Step 5: Match predicted text with expected labels using similarity threshold\n",
    "# def match_labels_with_similarity(extracted_data, fields, threshold=0.6):\n",
    "#     label_positions = {}\n",
    "#     for label, arabic_label in fields.items():\n",
    "#         french_pos = None\n",
    "#         arabic_pos = None\n",
    "#         for item in extracted_data:\n",
    "#             text = item[\"text\"].strip()\n",
    "            \n",
    "#             # Compare with French label\n",
    "#             if string_similarity(text, label) >= threshold:\n",
    "#                 french_pos = (item[\"center_x\"], item[\"center_y\"], item[\"bbox\"])\n",
    "            \n",
    "#             # Compare with Arabic label\n",
    "#             if string_similarity(text, arabic_label) >= threshold:\n",
    "#                 arabic_pos = (item[\"center_x\"], item[\"center_y\"], item[\"bbox\"])\n",
    "        \n",
    "#         if french_pos and arabic_pos:\n",
    "#             label_positions[label] = (french_pos, arabic_pos)\n",
    "#         else:\n",
    "#             print(f\"Could not match both French and Arabic labels for '{label}'\")\n",
    "    \n",
    "#     return label_positions\n",
    "\n",
    "# # Step 6: Crop the region between French and Arabic labels for each field\n",
    "# def crop_handwritten_regions(extracted_data, original_image):\n",
    "#     # Define the French labels and their Arabic translations\n",
    "#     fields = {\n",
    "#         \"Nom et prenom de l adherent\": \"إسم ولقب المنخرط\",\n",
    "#         \"Numero CIN ou passeport\": \"عدد بطاقة لتعريف الوطنية او جواذ لسفر\",\n",
    "#         \"Adresse de l'adherent\": \"عنوان المنخرط\",\n",
    "#         \"Matricule CNAM\": \"رقم بطاقة الصندوق الوطني للتأمين على المرض\",\n",
    "#         \"Matricule de l'adherent\": \"رقم المنخرط\",\n",
    "#         \"Nom et prenom du malade\": \"اسم و لقب المريض\",\n",
    "#         \"Date de naissance\": \"تاريخ الولادة\"\n",
    "#     }\n",
    "    \n",
    "#     # Create output folder\n",
    "#     output_folder = \"output_medical_form\"\n",
    "#     if not os.path.exists(output_folder):\n",
    "#         os.makedirs(output_folder)\n",
    "    \n",
    "#     # Match labels using similarity threshold\n",
    "#     label_positions = match_labels_with_similarity(extracted_data, fields, threshold=0.7)\n",
    "    \n",
    "#     # Crop the region between French and Arabic labels for each field\n",
    "#     for label, (french_pos, arabic_pos) in label_positions.items():\n",
    "#         french_x, french_y, french_bbox = french_pos\n",
    "#         arabic_x, arabic_y, arabic_bbox = arabic_pos\n",
    "        \n",
    "#         # Define the cropping region (between the French and Arabic labels)\n",
    "#         x1 = int(french_bbox[1][0])  # Right edge of French label\n",
    "#         x2 = int(arabic_bbox[0][0])  # Left edge of Arabic label\n",
    "#         y1 = min(int(french_bbox[0][1]), int(arabic_bbox[0][1]))  # Top edge\n",
    "#         y2 = max(int(french_bbox[2][1]), int(arabic_bbox[2][1]))  # Bottom edge\n",
    "\n",
    "#         # Apply 20px vertical padding\n",
    "#         height, width = original_image.shape[:2]\n",
    "#         padding = 20\n",
    "#         x1 = max(0, x1)\n",
    "#         x2 = min(width, x2)\n",
    "#         y1 = max(0, y1 - padding)\n",
    "#         y2 = min(height, y2 + padding)\n",
    "\n",
    "#         # Crop the region\n",
    "#         if x2 > x1 and y2 > y1:\n",
    "#             cropped_image = original_image[y1:y2, x1:x2]\n",
    "            \n",
    "#             # Save the cropped image\n",
    "#             output_filename = label.lower().replace(\" \", \"_\") + \".jpg\"\n",
    "#             output_path = os.path.join(output_folder, output_filename)\n",
    "#             cv2.imwrite(output_path, cropped_image)\n",
    "#             print(f\"Saved cropped image for '{label}' to {output_path}\")\n",
    "#         else:\n",
    "#             print(f\"Invalid cropping region for '{label}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fd52329-df1e-4514-bc6a-8a122829eaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_path = \"medical_form.jpg\"\n",
    "    \n",
    "#     # Preprocess the image\n",
    "# preprocessed_image, original_image = preprocess_image(image_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14b09dda-4b85-4c92-a41a-c01e5d3e7095",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "# # Extract text with EasyOCR\n",
    "# extracted_data = extract_text_with_easyocr(\"preprocessed_form.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee81a562-93b2-4acb-8fe7-0197d91586e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_data = merge_adjacent_regions(extracted_data, y_threshold=5, x_threshold=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "834ca663-037c-495c-9c90-548b4d613319",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print (extracted_data) \n",
    "# # Crop handwritten regions and save them\n",
    "# crop_handwritten_regions(extracted_data, original_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2947a7a-1228-45fb-bd08-1e5313d16ae7",
   "metadata": {},
   "source": [
    "## after fine-tunnig YOLOv8 model we are going to extract text using the bounding boxes predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed26d876-b8bb-4abf-b0e8-badaa0adfae9",
   "metadata": {},
   "source": [
    "## loading fine tuned model and creating the class names to be predicted as bounding box "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8925a839-a162-4114-bfe7-6bb6c274c1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the trained YOLOv8 model\n",
    "model_path = \"runs/detect/medical_form_yolo_best2/weights/best.pt\"\n",
    "model = YOLO(model_path)\n",
    "\n",
    "# Define class names (must match the order in data.yaml)\n",
    "class_names = [\n",
    "    \"nom et prenom de adherent\",\n",
    "    \"matricule cnam\",\n",
    "    \"matricule de adherent\",\n",
    "    \"addresse de ladherent\",\n",
    "    \"numero cin ou passeport\",\n",
    "    \"nom et prenom du malade\",\n",
    "    \"date de naissance\",\n",
    "    \"date\",\n",
    "    \"designation\",\n",
    "    \"honoraire\"\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77385b9b-978d-4041-a31f-04749598728b",
   "metadata": {},
   "source": [
    "## predicting bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "27d4fb05-e424-41c3-8afb-7f140dda3a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x480 1 nom et prenom de adherent, 1 matricule de adherent, 1 addresse de ladherent, 1 numero cin ou passeport, 1 nom et prenom du malade, 1 date de naissance, 2 dates, 1 designation, 2 honoraires, 151.6ms\n",
      "Speed: 2.4ms preprocess, 151.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    }
   ],
   "source": [
    "# Create output folder for annotated images and cropped regions\n",
    "output_folder = \"predictions\"\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# Path to the new image for prediction\n",
    "image_path = \"dataset/images/val/1432--5852526--20230720_page_0_1.jpg\"  # Replace with the path to your new image\n",
    "image = cv2.imread(image_path)\n",
    "if image is None:\n",
    "    raise FileNotFoundError(f\"Could not load image at {image_path}\")\n",
    "\n",
    "# Run inference\n",
    "results = model(image, imgsz=640, device=None)  # Use GPU (device=0); set to None for CPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7ce3f34e-cbe6-4b87-933b-89f023627e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence is : 0.8790407180786133\n",
      "Saved cropped region to predictions\\date_1.jpg\n",
      "Confidence is : 0.8396384119987488\n",
      "Saved cropped region to predictions\\designation_2.jpg\n",
      "Confidence is : 0.8309372067451477\n",
      "Saved cropped region to predictions\\nom_et_prenom_de_adherent_3.jpg\n",
      "Confidence is : 0.682129979133606\n",
      "Saved cropped region to predictions\\honoraire_4.jpg\n",
      "Confidence is : 0.6554888486862183\n",
      "Saved cropped region to predictions\\date_de_naissance_5.jpg\n",
      "Confidence is : 0.6499484777450562\n",
      "Saved cropped region to predictions\\nom_et_prenom_du_malade_6.jpg\n",
      "Confidence is : 0.6109005808830261\n",
      "Saved cropped region to predictions\\addresse_de_ladherent_7.jpg\n",
      "Confidence is : 0.608603835105896\n",
      "Saved cropped region to predictions\\honoraire_8.jpg\n",
      "Confidence is : 0.5044302940368652\n",
      "Saved cropped region to predictions\\numero_cin_ou_passeport_9.jpg\n",
      "Confidence is : 0.4248782694339752\n",
      "Saved cropped region to predictions\\matricule_de_adherent_10.jpg\n",
      "Confidence is : 0.29880237579345703\n",
      "Saved cropped region to predictions\\date_11.jpg\n",
      "Saved annotated image to predictions\\annotated_1432--5852526--20230720_page_0_1.jpg\n"
     ]
    }
   ],
   "source": [
    "# Process the detected bounding boxes\n",
    "crop_counter = 1\n",
    "for result in results:\n",
    "    boxes = result.boxes.xyxy  # Get bounding boxes in [x_min, y_min, x_max, y_max] format\n",
    "    confidences = result.boxes.conf  # Get confidence scores\n",
    "    class_ids = result.boxes.cls  # Get class IDs\n",
    "\n",
    "    for box, conf, cls_id in zip(boxes, confidences, class_ids):\n",
    "        # Extract bounding box coordinates\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        class_id = int(cls_id)\n",
    "        label = class_names[class_id]\n",
    "        confidence = float(conf)\n",
    "        print (f\"Confidence is : {confidence}\")\n",
    "        # # Apply 20px vertical padding for cropping\n",
    "        # height, width = image.shape[:2]\n",
    "        # padding = 0\n",
    "        # x1 = max(0, x1)\n",
    "        # x2 = min(width, x2)\n",
    "        # y1 = max(0, y1 - padding)\n",
    "        # y2 = min(height, y2 + padding)\n",
    "\n",
    "        # Crop the detected region\n",
    "        if x2 > x1 and y2 > y1:\n",
    "            cropped_image = image[y1:y2, x1:x2]\n",
    "            \n",
    "            # Save the cropped image\n",
    "            output_filename = f\"{label.replace(' ', '_')}_{crop_counter}.jpg\"\n",
    "            output_path = os.path.join(output_folder, output_filename)\n",
    "            cv2.imwrite(output_path, cropped_image)\n",
    "            print(f\"Saved cropped region to {output_path}\")\n",
    "            crop_counter += 1\n",
    "\n",
    "# Save the annotated image\n",
    "image_filename = os.path.basename(image_path)\n",
    "annotated_image_path = os.path.join(output_folder, f\"annotated_{image_filename}\")\n",
    "cv2.imwrite(annotated_image_path, image)\n",
    "print(f\"Saved annotated image to {annotated_image_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5637c292-4861-4290-b5e1-5fc37a234f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-large-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text from addresse_de_ladherent_7.jpg: ORBODY\n",
      "Extracted text from annotated_1432--5852526--20230720_page_0_1.jpg: 0 0\n",
      "Extracted text from date_1.jpg: 0 0\n",
      "Extracted text from date_11.jpg: 0 0\n",
      "Extracted text from date_de_naissance_5.jpg: 0 States\n",
      "Extracted text from designation_2.jpg: 0 0\n",
      "Extracted text from honoraire_4.jpg: 0 0\n",
      "Extracted text from honoraire_8.jpg: 0 0\n",
      "Extracted text from matricule_de_adherent_10.jpg: 0000386 \" c\n",
      "Extracted text from nom_et_prenom_de_adherent_3.jpg: Other ways.\n",
      "Extracted text from nom_et_prenom_du_malade_6.jpg: Tabel Kim Anna\n",
      "Extracted text from numero_cin_ou_passeport_9.jpg: 08368644.\n",
      "Saved extracted texts to predictions\\extracted_text.json\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "predictions_folder = \"predictions\"\n",
    "output_json_path = os.path.join(predictions_folder, \"extracted_text.json\")\n",
    "\n",
    "# Load the TrOCR model and processor\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-large-handwritten\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-large-handwritten\")\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Dictionary to store extracted text\n",
    "extracted_texts = {}\n",
    "\n",
    "# Iterate over all images in the predictions folder\n",
    "for image_name in os.listdir(predictions_folder):\n",
    "    if not image_name.lower().endswith((\".jpg\", \".png\", \".jpeg\")):\n",
    "        continue  # Skip non-image files (e.g., annotated images or JSON)\n",
    "\n",
    "    image_path = os.path.join(predictions_folder, image_name)\n",
    "    \n",
    "    try:\n",
    "        # Load the image\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "        # Preprocess the image\n",
    "        pixel_values = processor(image, return_tensors=\"pt\").pixel_values  # Shape: (1, 3, height, width)\n",
    "        pixel_values = pixel_values.to(device)\n",
    "\n",
    "        # Generate text\n",
    "        with torch.no_grad():  # Disable gradient computation for inference\n",
    "            generated_ids = model.generate(\n",
    "                pixel_values,\n",
    "                max_length=100,  # Adjust based on expected text length\n",
    "                num_beams=4,    # Beam search for better results\n",
    "                early_stopping=True\n",
    "            )\n",
    "\n",
    "        # Decode the generated IDs to text\n",
    "        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "        \n",
    "        # Store the extracted text\n",
    "        extracted_texts[image_name] = generated_text\n",
    "        print(f\"Extracted text from {image_name}: {generated_text}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_name}: {str(e)}\")\n",
    "        extracted_texts[image_name] = \"Error: Could not extract text\"\n",
    "\n",
    "# Save the extracted texts to a JSON file\n",
    "with open(output_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(extracted_texts, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Saved extracted texts to {output_json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2176f3-3257-4a3b-8a39-a8daa4dc2fee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
